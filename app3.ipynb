{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import joblib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the dataset\n",
    "diabetes_dataset = pd.read_csv('newDiabetes.csv')\n",
    "\n",
    "# Function for preprocessing data\n",
    "def preprocess_data(df):\n",
    "    # Create a copy to avoid modifying original data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Drop ID and No_Pation columns\n",
    "    df = df.drop(['ID', 'No_Pation'], axis=1)\n",
    "    \n",
    "    # Convert Gender to numeric\n",
    "    df['Gender'] = df['Gender'].map({'M': 0, 'F': 1})\n",
    "        # Handle CLASS (target variable)\n",
    "    df['CLASS'] = df['CLASS'].map({'N': 0, 'Y': 1, 'P': 2})\n",
    "    \n",
    "    # Check for any invalid values in numeric columns\n",
    "    numeric_columns = ['AGE', 'Urea', 'Cr', 'HbA1c', 'Chol', 'TG', 'HDL', 'LDL', 'VLDL', 'BMI']\n",
    "    df[numeric_columns] = df[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "    \n",
    "    # Remove rows with any invalid values\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Dataset Shape: (994, 12)\n",
      "\n",
      "Class Distribution:\n",
      " 1.0    839\n",
      "0.0    102\n",
      "2.0     53\n",
      "Name: CLASS, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Apply preprocessing\n",
    "processed_df = preprocess_data(diabetes_dataset)\n",
    "print(\"Processed Dataset Shape:\", processed_df.shape)\n",
    "print(\"\\nClass Distribution:\\n\", processed_df['CLASS'].value_counts())\n",
    "\n",
    "# Cell 3: Feature Engineering - Add polynomial features and interactions\n",
    "def engineer_features(df):\n",
    "    # Create a copy of dataframe\n",
    "    df_new = df.copy()\n",
    "    \n",
    "    # Create HbA1c to BMI ratio (important diabetes indicator)\n",
    "    df_new['HbA1c_BMI_ratio'] = df_new['HbA1c'] / df_new['BMI']\n",
    "    \n",
    "    # Create Cholesterol to HDL ratio (important cardiovascular risk factor)\n",
    "    df_new['Chol_HDL_ratio'] = df_new['Chol'] / df_new['HDL']\n",
    "    \n",
    "    # Create TG to HDL ratio (insulin resistance marker)\n",
    "    df_new['TG_HDL_ratio'] = df_new['TG'] / df_new['HDL']\n",
    "    \n",
    "    # Age group categorical feature\n",
    "    df_new['Age_Group'] = pd.cut(df_new['AGE'], \n",
    "                                bins=[0, 30, 45, 60, 100], \n",
    "                                labels=[0, 1, 2, 3])\n",
    "    \n",
    "    # BMI category (underweight, normal, overweight, obese)\n",
    "    df_new['BMI_Category'] = pd.cut(df_new['BMI'], \n",
    "                                   bins=[0, 18.5, 25, 30, 100], \n",
    "                                   labels=[0, 1, 2, 3])\n",
    "    \n",
    "    return df_new\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Enhanced Dataset Shape: (994, 17)\n",
      "\n",
      "New Features: ['HbA1c_BMI_ratio', 'Chol_HDL_ratio', 'TG_HDL_ratio', 'Age_Group', 'BMI_Category']\n"
     ]
    }
   ],
   "source": [
    "# Apply feature engineering\n",
    "enhanced_df = engineer_features(processed_df)\n",
    "print(\"\\nEnhanced Dataset Shape:\", enhanced_df.shape)\n",
    "print(\"\\nNew Features:\", [col for col in enhanced_df.columns if col not in processed_df.columns])\n",
    "\n",
    "# Cell 4: Split features and target\n",
    "X = enhanced_df.drop('CLASS', axis=1)\n",
    "y = enhanced_df['CLASS']\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected Features: ['AGE', 'HbA1c', 'BMI', 'HbA1c_BMI_ratio', 'BMI_Category']\n"
     ]
    }
   ],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Cell 5: Feature Selection\n",
    "# Use Random Forest for feature selection\n",
    "rf_selector = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select most important features\n",
    "selector = SelectFromModel(rf_selector, threshold='mean')\n",
    "selector.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Transform the data to include only selected features\n",
    "X_train_selected = selector.transform(X_train_scaled)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "# Get selected feature indices\n",
    "selected_indices = selector.get_support()\n",
    "selected_features = X.columns[selected_indices]\n",
    "print(\"\\nSelected Features:\", selected_features.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define parameters for Gradient Boosting\n",
    "gb_param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [2, 5],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize Gradient Boosting classifier\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "# Setup StratifiedKFold for more robust cross-validation\n",
    "stratified_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Gradient Boosting model...\n",
      "Fitting 5 folds for each of 72 candidates, totalling 360 fits\n"
     ]
    }
   ],
   "source": [
    "# Perform GridSearchCV\n",
    "gb_grid_search = GridSearchCV(\n",
    "    estimator=gb,\n",
    "    param_grid=gb_param_grid,\n",
    "    cv=stratified_cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Train the Gradient Boosting model\n",
    "print(\"\\nTraining Gradient Boosting model...\")\n",
    "gb_grid_search.fit(X_train_selected, y_train)\n",
    "\n",
    "# Get best Gradient Boosting model\n",
    "gb_best_model = gb_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with Gradient Boosting\n",
    "gb_y_pred = gb_best_model.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gradient Boosting Best Parameters: {'learning_rate': 0.01, 'max_depth': 5, 'min_samples_split': 2, 'n_estimators': 200, 'subsample': 1.0}\n",
      "\n",
      "Gradient Boosting Accuracy Score: 0.9698\n",
      "\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.94      0.80      0.86        20\n",
      "         1.0       0.97      0.99      0.98       168\n",
      "         2.0       1.00      0.91      0.95        11\n",
      "\n",
      "    accuracy                           0.97       199\n",
      "   macro avg       0.97      0.90      0.93       199\n",
      "weighted avg       0.97      0.97      0.97       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print Gradient Boosting results\n",
    "print(\"\\nGradient Boosting Best Parameters:\", gb_grid_search.best_params_)\n",
    "print(f\"\\nGradient Boosting Accuracy Score: {accuracy_score(y_test, gb_y_pred):.4f}\")\n",
    "print(\"\\nGradient Boosting Classification Report:\")\n",
    "print(classification_report(y_test, gb_y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Neural Network model...\n",
      "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=MLPClassifier(early_stopping=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.001, 0.01],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(50,), (100,), (50, 50),\n",
       "                                                (100, 50)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;max_iter&#x27;: [500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=MLPClassifier(early_stopping=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={&#x27;activation&#x27;: [&#x27;relu&#x27;, &#x27;tanh&#x27;],\n",
       "                         &#x27;alpha&#x27;: [0.0001, 0.001, 0.01],\n",
       "                         &#x27;hidden_layer_sizes&#x27;: [(50,), (100,), (50, 50),\n",
       "                                                (100, 50)],\n",
       "                         &#x27;learning_rate&#x27;: [&#x27;constant&#x27;, &#x27;adaptive&#x27;],\n",
       "                         &#x27;max_iter&#x27;: [500]},\n",
       "             scoring=&#x27;accuracy&#x27;, verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, random_state=42)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, random_state=42)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=StratifiedKFold(n_splits=5, random_state=42, shuffle=True),\n",
       "             estimator=MLPClassifier(early_stopping=True, random_state=42),\n",
       "             n_jobs=-1,\n",
       "             param_grid={'activation': ['relu', 'tanh'],\n",
       "                         'alpha': [0.0001, 0.001, 0.01],\n",
       "                         'hidden_layer_sizes': [(50,), (100,), (50, 50),\n",
       "                                                (100, 50)],\n",
       "                         'learning_rate': ['constant', 'adaptive'],\n",
       "                         'max_iter': [500]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define parameters for Neural Network\n",
    "nn_param_grid = {\n",
    "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [500]\n",
    "}\n",
    "# Initialize Neural Network classifier\n",
    "nn = MLPClassifier(random_state=42, early_stopping=True)\n",
    "\n",
    "# Perform GridSearchCV\n",
    "nn_grid_search = GridSearchCV(\n",
    "    estimator=nn,\n",
    "    param_grid=nn_param_grid,\n",
    "    cv=stratified_cv,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    scoring='accuracy'\n",
    ")\n",
    "\n",
    "# Train the Neural Network model\n",
    "print(\"\\nTraining Neural Network model...\")\n",
    "nn_grid_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Neural Network Best Parameters: {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100, 50), 'learning_rate': 'constant', 'max_iter': 500}\n",
      "\n",
      "Neural Network Accuracy Score: 0.8894\n",
      "\n",
      "Neural Network Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.50      0.95      0.66        20\n",
      "         1.0       0.98      0.94      0.96       168\n",
      "         2.0       0.00      0.00      0.00        11\n",
      "\n",
      "    accuracy                           0.89       199\n",
      "   macro avg       0.49      0.63      0.54       199\n",
      "weighted avg       0.88      0.89      0.88       199\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get best Neural Network model\n",
    "nn_best_model = nn_grid_search.best_estimator_\n",
    "\n",
    "# Make predictions with Neural Network\n",
    "nn_y_pred = nn_best_model.predict(X_test_scaled)\n",
    "\n",
    "# Print Neural Network results\n",
    "print(\"\\nNeural Network Best Parameters:\", nn_grid_search.best_params_)\n",
    "print(f\"\\nNeural Network Accuracy Score: {accuracy_score(y_test, nn_y_pred):.4f}\")\n",
    "print(\"\\nNeural Network Classification Report:\")\n",
    "print(classification_report(y_test, nn_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Voting Classifier...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a voting classifier\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', rf_selector),\n",
    "        ('gb', gb_best_model),\n",
    "        ('nn', nn_best_model)\n",
    "    ],\n",
    "    voting='soft'  # Use predicted probabilities\n",
    ")\n",
    "\n",
    "# Train the voting classifier\n",
    "print(\"\\nTraining Voting Classifier...\")\n",
    "# Need to handle feature dimension differences\n",
    "voting_clf.fit(X_train_scaled, y_train)  # Use scaled features for all\n",
    "\n",
    "# Make predictions with Voting Classifier\n",
    "voting_y_pred = voting_clf.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voting Classifier Accuracy Score: 0.9749\n",
      "\n",
      "Voting Classifier Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.90      0.95      0.93        20\n",
      "         1.0       0.98      1.00      0.99       168\n",
      "         2.0       1.00      0.64      0.78        11\n",
      "\n",
      "    accuracy                           0.97       199\n",
      "   macro avg       0.96      0.86      0.90       199\n",
      "weighted avg       0.98      0.97      0.97       199\n",
      "\n",
      "\n",
      "==== MODEL COMPARISON ====\n"
     ]
    }
   ],
   "source": [
    "# Print Voting Classifier results\n",
    "print(f\"\\nVoting Classifier Accuracy Score: {accuracy_score(y_test, voting_y_pred):.4f}\")\n",
    "print(\"\\nVoting Classifier Classification Report:\")\n",
    "print(classification_report(y_test, voting_y_pred))\n",
    "\n",
    "# Cell 9: Compare all models\n",
    "print(\"\\n==== MODEL COMPARISON ====\")\n",
    "models = {\n",
    "    \"Random Forest\": rf_selector,\n",
    "    \"Gradient Boosting\": gb_best_model,\n",
    "    \"Neural Network\": nn_best_model,\n",
    "    \"Voting Classifier\": voting_clf\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate models\n",
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    if model_name == \"Gradient Boosting\":\n",
    "        y_pred = model.predict(X_test_selected)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    report = classification_report(y_test, y_pred, output_dict=True)\n",
    "    \n",
    "    # Calculate weighted F1 score\n",
    "    f1 = report['weighted avg']['f1-score']\n",
    "    \n",
    "    return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Performance:\n",
      "                   Accuracy  F1 Score\n",
      "Voting Classifier  0.974874  0.972892\n",
      "Random Forest      0.969849  0.964554\n",
      "Gradient Boosting  0.969849  0.968888\n",
      "Neural Network     0.889447  0.876709\n"
     ]
    }
   ],
   "source": [
    "# Evaluate all models\n",
    "results = {}\n",
    "for name, model in models.items():\n",
    "    accuracy, f1 = evaluate_model(model, X_test_scaled, y_test, name)\n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'F1 Score': f1\n",
    "    }\n",
    "\n",
    "# Display results as a table\n",
    "results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "print(\"\\nModel Performance:\")\n",
    "print(results_df.sort_values('Accuracy', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Model: Voting Classifier with accuracy 0.9749\n"
     ]
    }
   ],
   "source": [
    "# Identify the best model\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "print(f\"\\nBest Model: {best_model_name} with accuracy {results_df.loc[best_model_name, 'Accuracy']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best model (Voting Classifier) saved as 'best_diabetes_model.joblib'\n",
      "Scaler saved as 'best_scaler.joblib'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Get the best model\n",
    "if best_model_name == \"Gradient Boosting\":\n",
    "    best_model = gb_best_model\n",
    "    # Save additional components needed for this model\n",
    "    joblib.dump(selector, 'feature_selector.joblib')\n",
    "    print(\"Feature selector saved as 'feature_selector.joblib'\")\n",
    "elif best_model_name == \"Random Forest\":\n",
    "    best_model = rf_selector\n",
    "elif best_model_name == \"Neural Network\":\n",
    "    best_model = nn_best_model\n",
    "else:\n",
    "    best_model = voting_clf\n",
    "# Save the best model\n",
    "joblib.dump(best_model, 'best_diabetes_model.joblib')\n",
    "joblib.dump(scaler, 'best_scaler.joblib')\n",
    "print(f\"\\nBest model ({best_model_name}) saved as 'best_diabetes_model.joblib'\")\n",
    "print(\"Scaler saved as 'best_scaler.joblib'\")\n",
    "\n",
    "# Cell 11: Function for making predictions\n",
    "def predict_diabetes(input_data=None):\n",
    "    \"\"\"\n",
    "    Make predictions for new data using the best model\n",
    "    \n",
    "    Parameters:\n",
    "    input_data (dict): Dictionary containing patient information\n",
    "    \n",
    "    Returns:\n",
    "    dict: Prediction results\n",
    "    \"\"\"\n",
    "    if input_data is None:\n",
    "        # Sample data\n",
    "        input_data = {\n",
    "            'ID': 421,\n",
    "            'No_Pation': 34227,\n",
    "            'Gender': 'M',\n",
    "            'AGE': 48,\n",
    "            'Urea': 4.7,\n",
    "            'Cr': 47,\n",
    "            'HbA1c': 4,\n",
    "            'Chol': 2.9,\n",
    "            'TG': 0.8,\n",
    "            'HDL': 0.9,\n",
    "            'LDL': 1.6,\n",
    "            'VLDL': 0.4,\n",
    "            'BMI': 24\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        # Convert input to DataFrame\n",
    "        input_df = pd.DataFrame([input_data])\n",
    "        \n",
    "        # Preprocess input\n",
    "        input_df = preprocess_data(input_df)\n",
    "        \n",
    "        # Apply feature engineering\n",
    "        input_df = engineer_features(input_df)\n",
    "        \n",
    "        # Scale features\n",
    "        input_scaled = scaler.transform(input_df)\n",
    "        \n",
    "        # Apply feature selection if needed\n",
    "        if best_model_name == \"Gradient Boosting\":\n",
    "            input_scaled = selector.transform(input_scaled)\n",
    "        # Make prediction\n",
    "        prediction = best_model.predict(input_scaled)\n",
    "        probabilities = best_model.predict_proba(input_scaled)\n",
    "        \n",
    "        # Map prediction to class\n",
    "        class_map = {0: 'Non-Diabetic', 1: 'Diabetic', 2: 'Pre-Diabetic'}\n",
    "        result = class_map[prediction[0]]\n",
    "        \n",
    "        # Create probability dictionary\n",
    "        prob_dict = {}\n",
    "        for i, label in enumerate(['Non-Diabetic', 'Diabetic', 'Pre-Diabetic']):\n",
    "            if i < len(probabilities[0]):\n",
    "                prob_dict[label] = probabilities[0][i]\n",
    "            else:\n",
    "                prob_dict[label] = 0.0\n",
    "        return {\n",
    "            'prediction': result,\n",
    "            'probabilities': prob_dict\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error making prediction: {str(e)}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample Prediction Result:\n",
      "Error making prediction: 'CLASS'\n"
     ]
    }
   ],
   "source": [
    "# Test the model with sample data\n",
    "sample_result = predict_diabetes()\n",
    "print(\"\\nSample Prediction Result:\")\n",
    "if isinstance(sample_result, dict):\n",
    "    print(f\"Prediction: {sample_result['prediction']}\")\n",
    "    print(\"\\nProbabilities:\")\n",
    "    for class_name, prob in sample_result['probabilities'].items():\n",
    "        print(f\"{class_name}: {prob:.4f}\")\n",
    "else:\n",
    "    print(sample_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
